# 一元线性回归

回归分析是确定两种或者两种以上变量间相互依赖的定量关系的一种统计分析方法。在回归分析中，只包括一个自变量和一个因变量，且二者的关系可以用一条直线近似表示，这种回归分析成为一元线性回归分析。


|广告费|4|8|9|8|7|12|6|10|6|9|
|---|---|---|---|---|---|---|---|---|---|---|
|销售额|9|20|22|15|17|23|18|25|10|20|

1. 然后通过在坐标轴上描出所有的点。
2. 然后我们需要做的就是求出一条直线，让 `(Y1实际值 - Y2预测值) ^ 2 + ...`的和最小，这个和称为SSE。也就是`Q(a,b) = ∑(Yi - a(Xi + b)) ^ 2`值最小的情况下求`a`和`b`。
3.  展开之后得到公式如下

	```
	a = ((X平均值 * Y平均值) - (X * Y)平均值) / (X平均值) ^ 2 - (X ^ 2)平均值
	b = Y平均值 - a * X平均值
	```
	求出`a`和`b`, `a = 1.98`, `b = 2.25`

4. 预测当X为2时，收入为6.2万元。
5. 接下来讨论下如何评价回归线拟合程度的好坏。
	
	因为通过求出a和b画出的线仅仅是一个近似，因为肯定很多的点都没有落在直线上。在统计学中R ^ 2称为拟合优度，用来判断回归方程的拟合程度。
	
	首先要明确一下如下几个概念。
	
	- `SST`—— 总偏差平方和。因变量的实际值与因变量平均值的差的平方和，反映了因变量取值的总体波动情况。
		
		`SST = ∑(Y实际值 - Y平均值) ^ 2`
	
	- `SSR` —— 回归平方和	。 因变量的回归值与平均值的差的平方和。反映了y的总偏差中由于x与y之间的线性关系引起的变化部分，是可以由回归直线来解释的。

		`SSR = ∑(Y回归值 - Y平均值) ^ 2`
		
	- `SSE` —— 残差平方和。 因变量的各实际观测值和回归值的差的平方和。反映了除了x对y的线性影响之外的其他因素对y变化的作用，是不能由回归直线来解释的。	
		`SSR = ∑(Y实际值 - Y回归值) ^ 2`
		
	- `SST = SSR + SSE`

	直线的拟合程度的好坏， 就是看这条直线能够多大程度反映Y值的变化。 也就是 `R ^ 2 = SSR / SST`, `R ^ 2` 的取值在 `0 - 1`之间，越接近1说明拟合程度越好。

	假如所有的点都在回归线上，说明SSE为0, 也就是R ^ 2 = 1, 意味着 Y 的变化 100%和 X有关， 没有其他因素会影响Y。 如果R ^ 2很低， 说明X 和 Y之间可能不存在线性关系。
	
6. 接下来进行变量的显著性检验。

	变量的显著性检验的目的：剔除回归系数中不显著的解释变量（也就是X）。
	
	例：
		
	比如有一个口袋里面装了黑白两种颜色的球一共20个，然后你想知道黑白球数量是否一致，那么如果用假设检验的思路就是这样做：首先假设黑白数量一样，然后随机抽取10个球，但是发现10个都是白的，如果最开始假设黑白数量一样是正确的，那么一下抽到10个白的的概率是很小的，但是这么小概率的事情居然发生了，所以我们有理由相信假设错误，黑白的数量应该是不一样的。
	
	理论：
	
	1. T检验用于对某一个自变量Xi对于Y的线性显著性，如果某一个Xi不显著，意味着可以从模型中剔除这个变量，使得模型更简洁。	
	2. F检测用于对所有的自变量X在整体上看对于Y的线性显著性。
	3. T检验的结果看`P-value`，F检验看`Significant F`值，一般要小于`0.05`，越小越显著（这个0.05其实是显著性水平，是人为设定的，如果比较严格，可以定成0.01，但是也会带来其他一些问题，不细说了）

	一般来说，只要F检验和关键变量的T检验通过了，模型的预测能力就是OK的。
					